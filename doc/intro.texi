
@page
@node    Introduction
@chapter Introduction
@cindex  Introduction

Complexity measurement tools provide several pieces of information.
They help to:

@enumerate
@item
locate suspicious areas in unfamiliar code
@item
get an idea of how much effort may be required to understand that code
@item
get an idea of the effort required to test a code base
@item
provide a reminder to yourself.  You may see what you've written as obvious,
but others may not.  It is useful to have a hint about what code may
seem harder to understand by others, and then decide if some rework
may be in order.
@end enumerate

But why another complexity analyzer?  Even though the McCabe analysis
tool already exists (@code{pmccabe}), I think the job it does is too
rough for gauging complexity, though it is ideal for gauging the
testing effort.  Each code path should be tested and the @code{pmccabe}
program provides a count of code paths.  That, however, is not the
only issue affecting human comprehension.  This program attempts to
take into account other factors that affect a human's ability to
understand.

@menu
* code length::             Code Length
* switch statement::        Switch Statement
* logic conditions::        Logic Conditions
* personal experience::     Personal Experience
* rationale summary::       Rationale Summary
* parsing::                 Parsing Method
* scoring algorithm::       Complexity Measurement Algorithm
@end menu

@node    code length
@section Code Length
Since @code{pmccabe} does not factor code length into its score, some folks
have taken to saying either long functions or a high McCabe score find
functions requiring attention.  But it means looking at two factors
without any visibility into how the length is obfuscating the code.

The technique used by this program is to count 1 for each line that a
statement spans, plus the complexity score of control expressions
(@code{for}, @code{while}, and @code{if} expressions).  The value for
a block of code is the sum of these multiplied by a nesting factor
(@pxref{complexity nesting-penalty}).  This score is then added to the
score of the encompassing block.  With all other things equal, a
procedure that is twice as long as another will have double the score.
@code{pmccabe} scores them identically.

@node    switch statement
@section Switch Statement
@code{pmccabe} has changed the scoring of @code{switch}
statements because they seemed too high.  @code{switch} statements
are now ``free'' in this new analysis.  That's wrong, too.
The code length needs to be counted and the code within a @code{switch}
statement adds more to the difficulty of comprehension than code at
a shallower logic level.

This program will multiply the score of the @code{switch} statement
content by the @xref{complexity nesting-penalty, nesting score factor}.

@node    logic conditions
@section Logic Conditions

@file{pmccabe} does not score logic conditions very well.
It overcharges for simple logical operations, it doesn't charge for
comma operators, and it undercharges for mixing assignment operators
and relational operators and the @code{and} and @code{or} logical
operators.

For example:
@example
xx = (A && B) || (C && D) || (E && F);
@end example
scores as @code{6}.  Strictly speaking, there are, indeed, six code
paths there.  That is a fairly straight forward expression that is not
nearly as complicated as this:
@example
  if (A) @{
    if (B) @{
      if (C) @{
        if (D)
          a-b-c-and-d;
      @} else if (E) @{
          a-b-no_c-and-e;
      @}
    @}
  @}
@end example
@noindent
and yet this scores exactly the same.  This program reduces the cost
to very little for a sequence of conditions at the same level.  (That
is, all @code{and} operators or all @code{or} operators.)  so the raw score
for these examples are 4 and 35, respectively (1 and 2 after scaling,
@pxref{complexity scale, @code{--scale}}).

If you nest boolean expressions, there is a little cost, assuming you
parenthesize grouped expressions so that @code{and} and @code{or}
operators do not appear at the same parenthesized level.  Also
assuming that you do not mix assignment and relational and boolean
operators all together.  If you do not parenthesize these into
subexpressions, their small scores get multiplied in ways that
sometimes wind up as a much higher score.

The intent here is to encourage easy to understand boolean expressions.
This is done by,
@itemize 
@item
not combining them with assignment statements
@item
canonicalizing them (two level expressions with all @code{&&}
operators at the bottom level and all @code{||} operators in the
nested level -\- or vice versa)
@item
parenthesizing for visual clarity (relational operations parenthesized
before being joined into larger @code{&&} or @code{||} expressions)
@item
breaking them up into multiple @code{if} statements, if convenient.
@end itemize

@node    personal experience
@section Personal Experience

I have used @code{pmccabe} on a number of occasions.  For a first
order approximation, it does okay.  However, I was interested in
zeroing in on the modules that needed the most help and there were a
lot of modules needing help.  I was finding I was looking at some
functions where I ought to have been looking at others.  So, I put
this together to see if there was a better correlation between what
seemed like hard code to me and the score derived by an analysis tool.

This has worked much better.  I ran @code{complexity} and
@code{pmccabe} against several million lines of code.  I correlated
the scores.  Where the two tools disagreed noticeably in relative
ranking, I took a closer look.  I found that @file{complexity} did,
indeed, seem to be more appropriate in its scoring.

@node    rationale summary
@section Rationale Summary

Ultimately, complexity is in the eye of the beholder and, even,
the particular mood of the beholder, too.  It is difficult to
tune a tool to properly accommodate these variables.

@code{complexity} will readily score as zero functions that are
extremely simple, and code that is long with many levels of logic
nesting will wind up scoring much higher than with @code{pmccabe}, barring
extreme changes to the default values for the tunables.

I have included several adjustments so that scores can be
tweaked to suit personal taste or gathered experience.
(@xref{complexity nesting-penalty, nesting score factor}, and
@ref{complexity demi-nesting-penalty, nested expression scoring factor},
but also @xref{complexity scale, normalization scaling factor},
to adjust scores to approximate scores rendered by @code{pmccabe}).

@node     parsing
@section  Parsing Method
@cindex   parsing

The method chosen for parsing the source has an effect on what gets
seen (scored) by the program.

@menu
* complexity parsing::  Complexity Measurement Parsing
* post-pp parsing::     Post-PreProcessing Parsing
* during pp parsing::   During PreProcessing Parsing
* pmccabe parsing::     @code{pmccabe} Parsing
@end menu

@node       complexity parsing
@subsection Complexity Measurement Parsing

This program examines the actual source a human looks at when the
file is opened, provided it is not pre-processed by @code{unifdef},
@xref{complexity unifdef, @code{unifdef}}.  This was chosen because
uncompiled code adds to the complexity of what a human must understand.
However, sometimes the source will contain unbalanced braces a la:
@example
#if FOO
  for (int ix = foo;;) @{
#else
  for (int ix = bar;;) @{
#endif
    code...
  @}
@end example
rendering code that cannot be parsed correctly.  @code{unifdef}-ing
makes it parsable.  Unfortunately, because the practice of @code{ifdef}-ing
unbalanced curly braces is so common, this program cannot rely on
finding the correct closing brace.

@strong{CAVEAT}: for the purposes of this program, procedures end
when either a matching closing brace is found @emph{or} a closing curly brace
is found in column 1, whichever comes first.  If the closing brace
in column one does not match the procedure opening brace, the
procedure is considered unscorable.

Fortunately, unscorable procedures are relatively unusual.

@strong{CAVEAT2}: K&R procedure headers are not recognized.
If anything other than an opening curly brace appears after
the parameter list will cause the code recognizer to go back
into ``look for a procedure header'' mode.  K&R procedures
are not just not scored, they are completely ignored.

This should probably get fixed, though.

@node       post-pp parsing
@subsection Post-PreProcessing Parsing

Another approach would be to use the @code{C} compiler and analize the
tokens coming out of the preprocessor.  The drawbacks are that macro
expansions will add to the complexity, even though they do not add to
human perceived complexity, and uncompiled code do not add to the
complexity measure.  The benefit, of course, is that you know for
certain where a procedure body starts and ends.

@node       during pp parsing
@subsection During PreProcessing Parsing

This would require going into the C preprocessor code and cause macros
to not be expanded.  Again, the great benefit is that you know for
certain you can find the starting and ending braces for every
procedure body.  The downsides are the extra work and, again, the
uncompiled code won't get counted in the complexity measure.

This might be a useful exercise to do some day, just to see
how helpful it might be.  Being able to recognize all procedure
bodies without fail would be a good thing.

@node       pmccabe parsing
@subsection @code{pmccabe} Parsing

The @code{pmccabe} parsing actually inspired the method for this program.
Thd difference is that @code{pmccabe} will always keep scanning until
a procedure body's closing curly brace is found, even if that means
counting the code from several following procedure definitions.
The consequence of this is that this program's code will see some
procedures that @code{pmccabe} will not, and vice versa.

@node    scoring algorithm
@section Complexity Measurement Algorithm

Fundamentally, this program counts non-comment source lines and
examines elements of parenthesized expressions.  This score
is multiplied by a nesting scoring factor for each layer of code nesting.

A parenthesized expression is scanned for operators.  If they are all
arithmetic operators, or all arithmetic and one relational operator,
the score is zero.  If all the operators are boolean @code{and}s or
they are all @code{or}s, then the score is one.  An assignment
operator with arithmetic operators also scores one.  If you mix
relational operators and all @code{and}s or all @code{or}s, the score
is the number of boolean elements.  If you mix @code{and}s and
@code{or}s at the same parenthetical level, the two counts are
multiplied, unless the boolean element count is higher.

Fundamentally, do not use multiple relational or boolean operators at
the same parenthetical level, unless they are all boolean @code{and}s
or they are all boolean @code{or}s.  If you use boolean operators and
relational operators in one expression, you are charged one statement
for each boolean element.

After scoring each statement and any parenthesized expressions, the
score is multiplied by any encompassing controlled block and added to
the score of that block.  A ``controlled block'' is a curly-braced
collection of statements controlled by one of the statement
controlling statements @code{do}, @code{for}, @code{else}, @code{if},
@code{switch}, or @code{while}.  Stand alone blocks for scoping local
variables do not trigger the multiplier.

You may trace the scores of parenthesized expressions and code
blocks (@pxref{complexity trace, trace output file}).  You will see
the raw score of the code block or expression.

The final score is the outermost score divided by the ``scaling factor'',
@xref{complexity scale, complexity scaling factor}.
